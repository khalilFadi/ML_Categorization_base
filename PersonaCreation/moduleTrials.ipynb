{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\khamad\\Documents\\GitHub\\ML_Categorization_base\\ml_env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text2text-generation\", model=\"NeuralNerd/t5-base-story-title-generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputCase = \"I am giving you information about a persona age, state, and gender as well as some recomendation the persona gave about the event, give me background information about the persona try to give it background info based on the responses: \\n Information used for character: “Personality: 'Title': '0_would_people_time_like','age': 18.212593296955305,'gender': 'Male','state': 'Idaho','name': 'David Dunlap''Responses':'There were so many volunteers for the service projects that they finished much too fast.  My group only took thirty minutes.  I suggest having a list of more service projects handy so that when a group finishes and has more time they can go to the next one.', 'More preparations in the planning stage. That would help with the organization, herding the crowd, and smoother activities. The speed dating for one was almost a flop with the little time, lots of people, and abrupt cut offs.', 'Only thing would be the service project, if there were more options for ways to serve. There were plenty of people and often times we were just standing around.', 'Some sort of smaller group participation ', Sing all the hymn verses please I like them a lot. Better directions to housing - I got lost really easily because the address wasn't accurate., 'Only thing would be the service project, if there were more options for ways to serve. There were plenty of people and often times we were just standing around.', 'Host it somewhere we don’t have to drive as much, such as at the BYU-I campus, closer to the housing.', 'More get to know you games', Better communication in general. I missed my original Carthage slot because the departure time was not clearly marked on the calendar in relation to the slot. I know I wasn't the only one to find it confusing either. The information about the Variety Show was also spotty and hard to follow, like where we would meet before we performed, a call time (There wasn't one), or how the rehearsal would go. I knew that there was a shuttle bus around Nauvoo, but I couldn't find information about it. The other thing I missed was physical copies of things like the schedule, maps, and basic information. I admire the effort to be paperless, but it made the conference harder to navigate. And I like that those sorts of things can be souvenirs of the conference too,, 'The Carthage bus and communication. I went to Carthage on Thursday. The bus arrived 45 minutes late, this was also after we were told by EFY staff that the bus had already left. We arrive at Carthage and just barely started the tour at 11:45 when a message was sent out that the buses were taking a lunch hour. This would’ve been fine except the fact that it would mean we’d miss lunch. I reached out to EFY staff and they promised that a bus was still coming for us during that time period and that they’d set aside lunch for us just in case. Almost 2 hours passed before the bus got us. We didn’t get back into Nauvoo until 2:45ish and I had missed my class and my temple session. Overall the communication was poor, especially regarding Carthage.'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (778 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'The Carthage Bus and Communication. I went to Carthage on Thursday and the bus arrived 45 minutes late. I was able to get to the bus and the bus was still there. I was able to get to the bus and the bus was there. I was able to get to the bus and the bus was there. I was able to get to the bus and the bus was there. I was able to get to the bus and the bus was there. I was able to get to the bus and the bus was                                                                                  . \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \"'}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(inputCase, min_length=400, max_length=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (778 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"David Dunlap, age 18.212593296955305, is an Idaho resident and has attended the conference for four years. He says that in addition to the service projects they have had 'more preparations in the planning stage. That would help with the organization, herding the crowd, and smoother activities.' David thinks the variety show was poorly run and that they made it very confusing - that is something he's not the biggest fan of, as he's a fan of free things, and there weren't any calls for rehearsals for the show, or for where they met before the show. - and the quality of the entertainment - I'm a fan of a good mix of bands and singers. - I felt that the events had the capacity to be quite enjoyable and interesting for the most part. He is not entirely disappointed in what he has personally witnessed, but rather that it wasn't well managed. He says that if something is under the radar for too long, or when something is being promoted too much, he's just not sure it's going to work. David also says that the organization seemed sloppy because when they have the conference there are so many activities to keep things organized so that when the events are done, things can get messy or a 'lost in translation' and there was too much confusion in planning. Generally speaking David thinks that the conference has 'more preparation in the planning stage' so it should be more clear of when it is going to be done, or at the very least, a clear guide of what it's supposed to be done, like the schedule, maps, etc. He likes that the event would be paperless for a lot of different reasons- he likes that there's little to no paperwork for people to sign and that the organization just doesn't seem to care about giving anyone any sort of physical copies of the info.\"}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text2text-generation\", model=\"google/flan-t5-large\")\n",
    "pipe(inputCase, min_length=300, max_length=1000, temperature = 1.2, top_p=0.9, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\khamad\\Documents\\GitHub\\ML_Categorization_base\\ml_env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer\n",
    "# Initialize the pipeline\n",
    "pipe = pipeline(\"text2text-generation\", model=\"facebook/blenderbot-400M-distill\")\n",
    "\n",
    "# Load the tokenizer for manual control\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/blenderbot-400M-distill\")\n",
    "\n",
    "# Example input text\n",
    "inputCase = inputCase\n",
    "\n",
    "# Tokenize the input, ensuring truncation if it exceeds the model's maximum length\n",
    "encoded_input = tokenizer(inputCase, truncation=True, max_length=1024, return_tensors=\"pt\")\n",
    "\n",
    "# Pass the tokenized input to the pipeline and limit output length to avoid IndexError\n",
    "output = pipe(\n",
    "    inputCase, \n",
    "    min_length=300, \n",
    "    max_length=1024,  # Ensure max_length doesn't exceed model's limits\n",
    "    temperature=1.2, \n",
    "    top_p=0.9, \n",
    "    do_sample=True\n",
    ")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5698724ce87a42d18d2310d290cbc9a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/956 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\khamad\\Documents\\GitHub\\ML_Categorization_base\\ml_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\khamad\\.cache\\huggingface\\hub\\models--RUCAIBox--mvp-story. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df649fd520f64a9da09709f61a24cf78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.87G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfd65f24f7f54f7eb30ad3bef0b3c40a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/27.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a22850a6040d4d81991aded995a9d9bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79c2d29d785042b19b2555e18a24c5b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e31bc9f0bf1146b19d24359b65081f82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d6c87af75044c38bdcf7479c23b1106",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/41.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\khamad\\Documents\\GitHub\\ML_Categorization_base\\ml_env\\Lib\\site-packages\\transformers\\generation\\utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'There were so many volunteers for the service projects that they finished much too fast.\\n'}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text2text-generation\", model=\"RUCAIBox/mvp-story\")\n",
    "pipe(inputCase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
